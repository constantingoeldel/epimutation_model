{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch import tensor\n",
    "import polars as pl\n",
    "import time\n",
    "\n",
    "class Sites(Dataset):\n",
    "\n",
    "    chrsm_lengths = {\n",
    "        \"1\": 30427671,\n",
    "        \"2\": 19698289,\n",
    "        \"3\": 23459830,\n",
    "        \"4\": 18585056,\n",
    "        \"5\": 26975502,\n",
    "    }\n",
    "\n",
    "    def chrsms(self):\n",
    "        return [1, 2, 3, 4] if self.mode == 'train' else [5]\n",
    "\n",
    "    def __init__(self, mode='train', width = 256):\n",
    "        self.mode = mode\n",
    "        self.width = width\n",
    "        self.features = pl.DataFrame()\n",
    "        self.labels = pl.DataFrame()\n",
    "        for chrsm in self.chrsms():\n",
    "            df =  pl.read_parquet(f\"./embeddings/chr_{chrsm}.parquet\")\n",
    "            self.features = self.features.vstack(df)\n",
    "\n",
    "        for chrsm in self.chrsms():\n",
    "            df =  pl.read_parquet(f\"./labels/{chrsm}.parquet\")\n",
    "            self.labels = self.labels.vstack(df)\n",
    "        self.labels = self.labels.drop([\"sequence\", \"std_st\"])\n",
    "\n",
    "\n",
    "        self.features = self.features.to_numpy()\n",
    "        self.labels = self.labels.to_numpy()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - 2 * self.width\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx\n",
    "        end = idx + 2 * self.width\n",
    "        f = self.features[start:end]\n",
    "        l = self.labels[idx+self.width]\n",
    "\n",
    "        f = torch.tensor(f, device=device, dtype=torch.float32).reshape(-1)\n",
    "        l = torch.squeeze(torch.tensor(l, device=device, dtype=torch.float32))\n",
    "\n",
    "        return f, l\n",
    "    \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = Sites(mode='train')\n",
    "test_dataset = Sites(mode='test')\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MethylationMaster(nn.Module):\n",
    "\n",
    "    def steady_state(alpha, beta):\n",
    "      \n",
    "        pi1 = lambda a, b:  (a * ((1.0 - a) ** 2  - (1.0 - b)**(2) - 1.0)) / ((a + b) * ((a + b - 1.0)**(2) - 2.0))\n",
    "        pi2 = lambda a, b:  (4.0 * a * b * (a + b - 2.0)) / ((a + b) * ((a + b - 1.0)**(2) - 2.0));\n",
    "    \n",
    "        return pi1(alpha, beta) + 0.5 * pi2(alpha, beta)\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=5120, nhead=80, batch_first=True)\n",
    "        # self.start = nn.Linear(5120, 512)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.linear = nn.Linear(5120, 2)\n",
    "        self.model = nn.Sequential(self.encoder, self.linear)\n",
    "        \n",
    "\n",
    "    def forward(self, features, targets = None):\n",
    "        logits =  self.model(features) # (B, (alpha, beta))\n",
    "       \n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "        loss = F.mse_loss(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # @torch.no_grad()\n",
    "    # def generate(self, idx):\n",
    "    #     logits = self.transformer(idx, idx) # (B * S, C)\n",
    "\n",
    "    #     last = logits[-1]\n",
    "    #     last_tar = targets[-1]\n",
    "\n",
    "    #     probs = F.softmax(last, dim=-1) \n",
    "    #     # sample from the distribution\n",
    "    #     guess = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "\n",
    "    #     print(f\"Guess: {guess}, correct: {last_tar}\")\n",
    "\n",
    "    #     return guess == last_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503.519234 M parameters\n"
     ]
    }
   ],
   "source": [
    "conschti = MethylationMaster().to(device)\n",
    "\n",
    "print(sum(p.numel() for p in conschti.parameters())/1e6, 'M parameters')\n",
    "optimizer = torch.optim.Adam(conschti.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2880323\n",
      "Step 0 (0.0000%), loss: 0.03843260183930397, alpha: 0.27837392687797546, beta: 0.11482207477092743\n",
      "Validation loss: 0.03843260183930397\n",
      "Step 1 (0.0000%), loss: 0.018323194235563278, alpha: 0.15977507829666138, beta: 0.13840101659297943\n",
      "Step 2 (0.0001%), loss: 0.022135496139526367, alpha: 0.23037387430667877, beta: 0.06178037077188492\n",
      "Step 3 (0.0001%), loss: 0.016016550362110138, alpha: -0.08077594637870789, beta: 0.04442596808075905\n",
      "Step 4 (0.0001%), loss: 0.014578926376998425, alpha: 0.07141273468732834, beta: 0.07230084389448166\n",
      "Step 5 (0.0002%), loss: 0.02764589712023735, alpha: -0.04781973361968994, beta: -0.04489924758672714\n",
      "Step 6 (0.0002%), loss: 0.02301700785756111, alpha: 0.12388560175895691, beta: 0.06368600577116013\n",
      "Step 7 (0.0002%), loss: 0.016085147857666016, alpha: 0.27394604682922363, beta: 0.15755964815616608\n",
      "Step 8 (0.0003%), loss: 0.04460945725440979, alpha: -0.1817365437746048, beta: -0.09079090505838394\n",
      "Step 9 (0.0003%), loss: 0.018867190927267075, alpha: 0.1845487356185913, beta: 0.05725831538438797\n",
      "Step 10 (0.0003%), loss: 0.02422797866165638, alpha: 0.23722106218338013, beta: 0.0559978261590004\n",
      "Step 11 (0.0004%), loss: 0.021369900554418564, alpha: 0.001942281611263752, beta: -0.08172571659088135\n",
      "Step 12 (0.0004%), loss: 0.017523210495710373, alpha: 0.0839444100856781, beta: 0.06540905684232712\n",
      "Step 13 (0.0005%), loss: 0.02796284854412079, alpha: -0.06819034367799759, beta: -0.038870275020599365\n",
      "Step 14 (0.0005%), loss: 0.023929860442876816, alpha: 0.13136768341064453, beta: 0.004822974093258381\n",
      "Step 15 (0.0005%), loss: 0.019191520288586617, alpha: -0.09841268509626389, beta: -0.05495282635092735\n",
      "Step 16 (0.0006%), loss: 0.03587336838245392, alpha: 0.2059388905763626, beta: 0.14289051294326782\n",
      "Step 17 (0.0006%), loss: 0.015036793425679207, alpha: -0.07413395494222641, beta: 0.04919736832380295\n",
      "Step 18 (0.0006%), loss: 0.030071107670664787, alpha: -0.14427919685840607, beta: -0.04221677780151367\n",
      "Step 19 (0.0007%), loss: 0.024603646248579025, alpha: 0.03271516039967537, beta: 0.011409069411456585\n",
      "Step 20 (0.0007%), loss: 0.02899855747818947, alpha: -0.3403599262237549, beta: -0.18247771263122559\n",
      "Step 21 (0.0007%), loss: 0.02094813622534275, alpha: 0.08557851612567902, beta: 0.0850268304347992\n",
      "Step 22 (0.0008%), loss: 0.017003025859594345, alpha: 0.19490773975849152, beta: 0.2565373182296753\n",
      "Step 23 (0.0008%), loss: 0.01774558797478676, alpha: 0.013423980213701725, beta: 0.03348983824253082\n",
      "Step 24 (0.0008%), loss: 0.022208724170923233, alpha: 0.17151769995689392, beta: 0.15743602812290192\n",
      "Step 25 (0.0009%), loss: 0.017807036638259888, alpha: -0.06580255180597305, beta: -0.08215368539094925\n",
      "Step 26 (0.0009%), loss: 0.01752845197916031, alpha: -0.18505625426769257, beta: -0.16385462880134583\n",
      "Step 27 (0.0009%), loss: 0.019619518890976906, alpha: -0.0270529817789793, beta: 0.07100946456193924\n",
      "Step 28 (0.0010%), loss: 0.02316221594810486, alpha: -0.06951389461755753, beta: -0.11796613037586212\n",
      "Step 29 (0.0010%), loss: 0.016153136268258095, alpha: -0.07830595970153809, beta: 0.00952332466840744\n",
      "Step 30 (0.0010%), loss: 0.01767367497086525, alpha: -0.10650412738323212, beta: 0.012468879111111164\n",
      "Step 31 (0.0011%), loss: 0.02022029645740986, alpha: -0.09185195714235306, beta: -0.10501576215028763\n",
      "Step 32 (0.0011%), loss: 0.02283625118434429, alpha: -0.22437238693237305, beta: -0.14211328327655792\n",
      "Step 33 (0.0011%), loss: 0.02210879698395729, alpha: 0.009121879003942013, beta: -0.07840778678655624\n",
      "Step 34 (0.0012%), loss: 0.016873065382242203, alpha: 0.1463332325220108, beta: 0.0959692895412445\n",
      "Step 35 (0.0012%), loss: 0.0194593146443367, alpha: -0.07285197824239731, beta: -0.09515321999788284\n",
      "Step 36 (0.0012%), loss: 0.019685033708810806, alpha: 0.26701104640960693, beta: 0.1218220666050911\n",
      "Step 37 (0.0013%), loss: 0.01666386052966118, alpha: -0.20585773885250092, beta: -0.009447401389479637\n",
      "Step 38 (0.0013%), loss: 0.01864698901772499, alpha: 0.06366994231939316, beta: 0.1630999892950058\n",
      "Step 39 (0.0014%), loss: 0.01934291422367096, alpha: 0.08425810188055038, beta: 0.11628088355064392\n",
      "Step 40 (0.0014%), loss: 0.02913825958967209, alpha: 0.004702660255134106, beta: -0.1361011564731598\n",
      "Step 41 (0.0014%), loss: 0.01539083942770958, alpha: 0.1342034935951233, beta: 0.10005874931812286\n",
      "Step 42 (0.0015%), loss: 0.01922684535384178, alpha: 0.01714005321264267, beta: 0.05000758543610573\n",
      "Step 43 (0.0015%), loss: 0.014172171242535114, alpha: -0.00955396331846714, beta: -0.0854441225528717\n",
      "Step 44 (0.0015%), loss: 0.02227715589106083, alpha: 0.012538258917629719, beta: 0.021926889196038246\n",
      "Step 45 (0.0016%), loss: 0.024418577551841736, alpha: -0.11544378846883774, beta: -0.07887469977140427\n",
      "Step 46 (0.0016%), loss: 0.011153768748044968, alpha: -0.3132361173629761, beta: -0.19041627645492554\n",
      "Step 47 (0.0016%), loss: 0.016414005309343338, alpha: 0.060346078127622604, beta: -0.007020921446382999\n",
      "Step 48 (0.0017%), loss: 0.023240575566887856, alpha: 0.12523598968982697, beta: -0.017699258401989937\n",
      "Step 49 (0.0017%), loss: 0.01692814566195011, alpha: -0.16060654819011688, beta: -0.0488741435110569\n",
      "Step 50 (0.0017%), loss: 0.009011901915073395, alpha: 0.28567469120025635, beta: 0.06387381255626678\n",
      "Step 51 (0.0018%), loss: 0.026896249502897263, alpha: 0.12424414604902267, beta: 0.017065376043319702\n",
      "Step 52 (0.0018%), loss: 0.02003391832113266, alpha: -0.22534844279289246, beta: -0.12704753875732422\n",
      "Step 53 (0.0018%), loss: 0.011098986491560936, alpha: 0.11076172441244125, beta: -0.019767481833696365\n",
      "Step 54 (0.0019%), loss: 0.018158674240112305, alpha: 0.27412664890289307, beta: 0.28437772393226624\n",
      "Step 55 (0.0019%), loss: 0.028102565556764603, alpha: 0.32320091128349304, beta: 0.1155584454536438\n",
      "Step 56 (0.0019%), loss: 0.02399633266031742, alpha: 0.19002605974674225, beta: -0.00704058725386858\n",
      "Step 57 (0.0020%), loss: 0.02446511760354042, alpha: 0.02081950381398201, beta: 0.03230113163590431\n",
      "Step 58 (0.0020%), loss: 0.02329515665769577, alpha: 0.04210984706878662, beta: 0.03509311005473137\n",
      "Step 59 (0.0020%), loss: 0.02116519771516323, alpha: 0.05043613910675049, beta: 0.1861424595117569\n",
      "Step 60 (0.0021%), loss: 0.030145732685923576, alpha: 0.0668092593550682, beta: 0.04883560910820961\n",
      "Step 61 (0.0021%), loss: 0.019302865490317345, alpha: 0.18854615092277527, beta: 0.17040307819843292\n",
      "Step 62 (0.0022%), loss: 0.023168029263615608, alpha: 0.07907873392105103, beta: 0.10016191750764847\n",
      "Step 63 (0.0022%), loss: 0.019933337345719337, alpha: -0.1674952507019043, beta: -0.12315944582223892\n",
      "Step 64 (0.0022%), loss: 0.02198750525712967, alpha: -0.2577251195907593, beta: -0.1459996998310089\n",
      "Step 65 (0.0023%), loss: 0.02520665153861046, alpha: -0.03159622848033905, beta: 0.0490153431892395\n",
      "Step 66 (0.0023%), loss: 0.018534211441874504, alpha: 0.021769464015960693, beta: 0.06705509126186371\n",
      "Step 67 (0.0023%), loss: 0.02058219537138939, alpha: -0.07450509816408157, beta: -0.12102434039115906\n",
      "Step 68 (0.0024%), loss: 0.0213228277862072, alpha: 0.09139639884233475, beta: 0.14655663073062897\n",
      "Step 69 (0.0024%), loss: 0.024818304926156998, alpha: -0.1280471384525299, beta: 0.028340931981801987\n",
      "Step 70 (0.0024%), loss: 0.015293573960661888, alpha: -0.05910084396600723, beta: -0.10632171481847763\n",
      "Step 71 (0.0025%), loss: 0.01715320721268654, alpha: 0.19729599356651306, beta: 0.07409344613552094\n",
      "Step 72 (0.0025%), loss: 0.029778916388750076, alpha: -0.05848894640803337, beta: 0.015464781783521175\n",
      "Step 73 (0.0025%), loss: 0.015525467693805695, alpha: 0.3361561596393585, beta: 0.1148659884929657\n",
      "Step 74 (0.0026%), loss: 0.03691966086626053, alpha: -0.0601406991481781, beta: -0.04575332999229431\n",
      "Step 75 (0.0026%), loss: 0.02070368453860283, alpha: 0.1945061832666397, beta: 0.014661265537142754\n",
      "Step 76 (0.0026%), loss: 0.03131386637687683, alpha: -0.4013601839542389, beta: -0.275980144739151\n",
      "Step 77 (0.0027%), loss: 0.01564648002386093, alpha: -0.10898583382368088, beta: -0.08985766768455505\n",
      "Step 78 (0.0027%), loss: 0.021586351096630096, alpha: 0.1341400295495987, beta: 0.10212768614292145\n",
      "Step 79 (0.0027%), loss: 0.01828116551041603, alpha: 0.12951993942260742, beta: 0.050834376364946365\n",
      "Step 80 (0.0028%), loss: 0.019930893555283546, alpha: -0.21145011484622955, beta: -0.14045508205890656\n",
      "Step 81 (0.0028%), loss: 0.017129406332969666, alpha: -0.09653948247432709, beta: -0.08750171959400177\n",
      "Step 82 (0.0028%), loss: 0.01977243646979332, alpha: 0.04643745720386505, beta: -0.02714267559349537\n",
      "Step 83 (0.0029%), loss: 0.022256240248680115, alpha: 0.22784624993801117, beta: 0.08828891068696976\n",
      "Step 84 (0.0029%), loss: 0.02593502402305603, alpha: -0.049586690962314606, beta: 0.09611321240663528\n",
      "Step 85 (0.0030%), loss: 0.027437467128038406, alpha: 0.03596285358071327, beta: 0.0838790088891983\n",
      "Step 86 (0.0030%), loss: 0.01723324880003929, alpha: -0.025459827855229378, beta: 0.03240518644452095\n",
      "Step 87 (0.0030%), loss: 0.0169028639793396, alpha: 0.16292135417461395, beta: 0.045748382806777954\n",
      "Step 88 (0.0031%), loss: 0.01814235746860504, alpha: 0.057337451726198196, beta: 0.12420429289340973\n",
      "Step 89 (0.0031%), loss: 0.023605292662978172, alpha: -0.010032938793301582, beta: 0.031221939250826836\n",
      "Step 90 (0.0031%), loss: 0.022108297795057297, alpha: -0.24162733554840088, beta: -0.22084468603134155\n",
      "Step 91 (0.0032%), loss: 0.021779997274279594, alpha: -0.1369691789150238, beta: -0.01829034835100174\n",
      "Step 92 (0.0032%), loss: 0.024903278797864914, alpha: -0.15437883138656616, beta: -0.037954170256853104\n",
      "Step 93 (0.0032%), loss: 0.02265043742954731, alpha: 0.10391512513160706, beta: 0.16527819633483887\n",
      "Step 94 (0.0033%), loss: 0.01932208612561226, alpha: -0.07743652164936066, beta: -0.016403527930378914\n",
      "Step 95 (0.0033%), loss: 0.027352269738912582, alpha: 0.26556918025016785, beta: 0.06385397911071777\n",
      "Step 96 (0.0033%), loss: 0.020601479336619377, alpha: 0.2317257672548294, beta: 0.15156620740890503\n",
      "Step 97 (0.0034%), loss: 0.01239458005875349, alpha: -0.0459599532186985, beta: 0.04162900894880295\n",
      "Step 98 (0.0034%), loss: 0.015766005963087082, alpha: -0.1548539251089096, beta: -0.07171932607889175\n",
      "Step 99 (0.0034%), loss: 0.02504054829478264, alpha: 0.23642386496067047, beta: 0.16691234707832336\n",
      "Step 100 (0.0035%), loss: 0.019297298043966293, alpha: -0.15507572889328003, beta: -0.19573275744915009\n",
      "Validation loss: 0.019297298043966293\n",
      "Step 101 (0.0035%), loss: 0.02463427558541298, alpha: -0.22374388575553894, beta: -0.176632821559906\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "test_interval = 100\n",
    "run_name = \"encoder_only_755M\"\n",
    "\n",
    "test_dataloader = iter(test_dataloader)\n",
    "num = len(train_dataloader)\n",
    "print(num)\n",
    "\n",
    "#reset log file\n",
    "with open(f\"log/{run_name}.txt\", \"w\") as f:\n",
    "    f.write(\"epoch,step,train_loss,test_loss\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (feature, label) in enumerate(train_dataloader): #batched\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, loss = conschti(feature , label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        percent_done = (i / num) * 100\n",
    "        percent_done = '{:.4f}'.format(percent_done)\n",
    "\n",
    "        print(f\"Step {i} ({percent_done}%), loss: {loss.item()}, alpha: {logits[0][0]}, beta: {logits[0][1]}, steady_state: {MethylationMaster.steady_state(logits[0][0], logits[0][1])}\")\n",
    "\n",
    "        if i % test_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                    feature, label = next(test_dataloader)\n",
    "                    logits, test_loss = conschti(feature , label)\n",
    "                    print(f\"Validation loss: {loss.item()}\")\n",
    "\n",
    "                    with open(f\"log/{run_name}.txt\", \"a\") as f:\n",
    "                      f.write(f\"{epoch},{i},{loss},{test_loss}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
